{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8831cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417dd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_HORIZON = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_prefix = \"data/ECG200/ECG200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# In order to perform 10-fold cross-validation, we\n",
    "# will merge the provided train and test splits and \n",
    "# we will split the data durign the cross-validation\n",
    "\n",
    "train_data_with_class_labels = np.genfromtxt(file_name_prefix+'_TRAIN.txt')\n",
    "test_data_with_class_labels = np.genfromtxt(file_name_prefix+'_TEST.txt')\n",
    "\n",
    "data_with_class_labels = np.vstack( (train_data_with_class_labels, \n",
    "                                     test_data_with_class_labels))\n",
    "data_without_class_labels = data_with_class_labels[:,1:]\n",
    "input_data = data_without_class_labels[:,:-FORECAST_HORIZON]\n",
    "target = data_without_class_labels[:,-FORECAST_HORIZON:]\n",
    "\n",
    "# We make sure that the length of the time series is a multiple of 4 \n",
    "\n",
    "NUM_INPUT_FEATURES = len(input_data[0]) \n",
    "values_to_cut = NUM_INPUT_FEATURES % 4\n",
    "if values_to_cut != 0:\n",
    "    input_data = input_data[:,values_to_cut:]\n",
    "    NUM_INPUT_FEATURES = NUM_INPUT_FEATURES - values_to_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw(ts1, ts2):\n",
    "    LEN_TS1 = len(ts1)\n",
    "    LEN_TS2 = len(ts2)\n",
    "    dtw_matrix = np.zeros( (LEN_TS1, LEN_TS2), dtype=float )\n",
    "    dtw_matrix[0,0] = abs(ts1[0]-ts2[0])\n",
    "    \n",
    "    for i in range(1, LEN_TS1):\n",
    "        dtw_matrix[i,0] = dtw_matrix[i-1,0]+abs(ts1[i]-ts2[0])\n",
    "        \n",
    "    for j in range(1, LEN_TS2):\n",
    "        dtw_matrix[0,j] = dtw_matrix[0,j-1]+abs(ts1[0]-ts2[j])\n",
    "\n",
    "    for i in range(1, LEN_TS1):\n",
    "        for j in range(1, LEN_TS2):\n",
    "            dtw_matrix[i,j] = min(dtw_matrix[i-1,j-1], dtw_matrix[i-1,j], \n",
    "                            dtw_matrix[i, j-1]) + abs(ts1[i]-ts2[j])\n",
    "            \n",
    "    return dtw_matrix[ len(ts1)-1, len(ts2)-1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_activations(data, convolutional_filters):\n",
    "    \"\"\"\n",
    "    Calculation of the activations of the distortion-aware convolutional layer.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data : np.array \n",
    "      Two-dimensional array containing the input data, \n",
    "      each row of the array corresponds to one of the time series\n",
    "    convolutional_filters : np.array\n",
    "      Three-dimensional array containing the parameters of the dynamic \n",
    "      convolutional layer. The first index corresponds to the output channel\n",
    "      of the convolution, the second index corresponds to the input channel \n",
    "      (the current implementation only works with 1 input channel, so the second\n",
    "      index is always zero), the third index is the position within the local\n",
    "      pattern corresponding to a convolutional filter\n",
    "    \"\"\"\n",
    "    num_instances = len(data)\n",
    "    length_of_time_series = len(data[0])\n",
    "    num_conv_filters = len(convolutional_filters)\n",
    "    conv_filter_size = len(convolutional_filters[0][0])\n",
    "\n",
    "    activations = np.zeros( (num_instances, 2*num_conv_filters, \n",
    "                           length_of_time_series-conv_filter_size+1) )\n",
    "    for i in range(num_instances):\n",
    "        for j in range(length_of_time_series-conv_filter_size+1):\n",
    "            for k in range(num_conv_filters):\n",
    "                activations[i,k,j] = 1.0/(1.0 + dtw(convolutional_filters[k][0],\n",
    "                                            data[i,j:j+conv_filter_size]))\n",
    "    for i in range(num_instances):\n",
    "        for j in range(length_of_time_series-conv_filter_size+1):\n",
    "            for k in range(num_conv_filters):\n",
    "                activations[i,num_conv_filters+k,j] = np.sum(convolutional_filters[k][0]*data[i,j:j+conv_filter_size])\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ea14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the neural networks\n",
    "\n",
    "CONV_FILTERS = 25\n",
    "CONV_FILTER_SIZE = 9\n",
    "\n",
    "\n",
    "class Net_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_CNN, self).__init__()\n",
    "        num_units_fc = 100\n",
    "        self.num_inputs_fc = int(CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = CONV_FILTERS, \n",
    "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(self.num_inputs_fc, num_units_fc)\n",
    "        self.out = nn.Linear(num_units_fc, FORECAST_HORIZON) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, NUM_INPUT_FEATURES)\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1, self.num_inputs_fc)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Please note that the distortion-aware convolutional layer is initialized  \n",
    "# using the parameters learned during the \"initial stage\" (in which a \"usual\" \n",
    "# convolutional network is trained). Once the initial stage is completed, \n",
    "# the parameters of the dynamic convoltuional layer are fixed, therefore,\n",
    "# the activations of the dynamic convolutional layer will be pre-calculated \n",
    "# outside the network for efficient implementation.\n",
    "\n",
    "class Net_DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_DCNN, self).__init__()\n",
    "        num_units_fc = 100\n",
    "\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(int(2*CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2), num_units_fc)\n",
    "        self.out = nn.Linear(num_units_fc, FORECAST_HORIZON) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 2*CONV_FILTERS, NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1,int(2*CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2))\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to evaluate the network \n",
    "def eval_net(net, test_input, test_target):\n",
    "    test_dataset = torch.utils.data.TensorDataset( \n",
    "        torch.Tensor(test_input), \n",
    "        torch.Tensor(test_target)\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    mae = 0\n",
    "    mse = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, true_target in testloader:\n",
    "            predicted_target = net(inputs).numpy()\n",
    "            true_target = true_target.numpy()\n",
    "\n",
    "            mae += float(np.mean(np.abs(predicted_target - true_target)))\n",
    "            mse += float(np.mean((predicted_target - true_target)**2))\n",
    "            total += 1\n",
    "\n",
    "    return mse/total, mae/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1dd8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used to distort time series\n",
    "\n",
    "def atomic_elongational_noise(ts):\n",
    "    n = len(ts)\n",
    "    pos_del = random.randint(0,n-1)\n",
    "    pos_elongate = random.randint(0,n-2)\n",
    "    ts = list(ts)\n",
    "    ts = ts[:pos_del] + ts[pos_del+1:] # delete a value\n",
    "    ts = ts[:pos_elongate+1] + ts[pos_elongate:] # elongate a value\n",
    "    return ts\n",
    "\n",
    "def elongational_noise(timeseries_dataset, noise_level):\n",
    "    timeseries_dataset_with_noise = []\n",
    "    for i in range(len(timeseries_dataset)):\n",
    "        ts = list(timeseries_dataset[i])\n",
    "        for j in range(noise_level):\n",
    "            ts = atomic_elongational_noise(ts)\n",
    "        timeseries_dataset_with_noise.append(ts)\n",
    "    return np.array(timeseries_dataset_with_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c0bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the main experimental loop\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "mse_cnn  = [[],[],[],[]]\n",
    "mse_dcnn = [[],[],[],[]]\n",
    "mae_cnn  = [[],[],[],[]]\n",
    "mae_dcnn = [[],[],[],[]]\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(input_data):\n",
    "    fold = fold + 1\n",
    "\n",
    "    train_data = input_data[train_index]\n",
    "    train_target = target[train_index]\n",
    "    test_data = input_data[test_index]\n",
    "    test_target = target[test_index]\n",
    "\n",
    "    # Training of CNN. This is simultaneously the initial stage of training of the DCNN.\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "      torch.Tensor(train_data), \n",
    "      torch.Tensor(train_target) \n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "      train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "    cnn = Net_CNN()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=1e-5)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_n = 0\n",
    "\n",
    "    print(\"Training CNN...\")\n",
    "\n",
    "    for epoch in range(500):  \n",
    "        for input_batch, target_batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction_batch = cnn(input_batch)\n",
    "\n",
    "            loss = criterion(prediction_batch, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_n = running_n + 1\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch: {:3d} loss: {:4.3f}\".format(epoch, running_loss/running_n))\n",
    "            running_loss = 0.0\n",
    "            running_n = 0\n",
    "\n",
    "    # Obtain the parameters of the dynamic convolutional layer, and \n",
    "    # precalculate its activations\n",
    "    params = []\n",
    "    for p in cnn.parameters():\n",
    "        params.append(p)\n",
    "\n",
    "    convolutional_filters = np.array(params[0].detach().numpy(), \n",
    "                                   dtype=float)\n",
    "\n",
    "    dc_activations_train = dc_activations(train_data, convolutional_filters)\n",
    "\n",
    "    # Train DCNN\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "      torch.Tensor(dc_activations_train), \n",
    "      torch.Tensor(train_target) \n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "      train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "\n",
    "    dcnn = Net_DCNN()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(dcnn.parameters(), lr=1e-5)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_n = 0\n",
    "\n",
    "    print(\"Training DCNN...\")\n",
    "\n",
    "    for epoch in range(500):  \n",
    "        for input_batch, target_batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction_batch = dcnn(input_batch) \n",
    "\n",
    "            loss = criterion(prediction_batch, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_n = running_n + 1\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch: {:3d} loss: {:4.3f}\".format(epoch, running_loss/running_n))\n",
    "            running_loss = 0.0\n",
    "            running_n = 0\n",
    "  \n",
    "\n",
    "    # Evaluation at different levels of distortion\n",
    "    \n",
    "    test_data1 = test_data_with_noise = elongational_noise(test_data, 1)\n",
    "    test_data5 = test_data_with_noise = elongational_noise(test_data, 5)\n",
    "    test_data10 = test_data_with_noise = elongational_noise(test_data, 10)\n",
    "    \n",
    "    testsets = [test_data, test_data1, test_data5, test_data10]\n",
    "    noise_level = [0, 1, 5, 10]\n",
    "    \n",
    "    for i in range(4):\n",
    "        a_mse, a_mae = eval_net(cnn, testsets[i], test_target)\n",
    "        mse_cnn[i].append(a_mse)\n",
    "        mae_cnn[i].append(a_mae)\n",
    "\n",
    "        dc_activations_test = dc_activations(testsets[i], convolutional_filters)\n",
    "        a_mse_dc, a_mae_dc = eval_net(dcnn, dc_activations_test, test_target)\n",
    "\n",
    "        mse_dcnn[i].append(a_mse_dc)\n",
    "        mae_dcnn[i].append(a_mae_dc)\n",
    "\n",
    "        print(f\"Fold: {fold:2d}, Distortion: {noise_level[i]}\")\n",
    "        print(f\"  aMSE of CNN:  {a_mse:6.4f}\")\n",
    "        print(f\"  aMSE of DCNN: {a_mse_dc:6.4f}\")\n",
    "        print(f\"  aMAE of CNN:  {a_mae:6.4f}\")\n",
    "        print(f\"  aMAE of DCNN: {a_mae_dc:6.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5303219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results, calculate p-values \n",
    "\n",
    "print(file_name_prefix.split('/')[-1])\n",
    "print(f\"Mean avgMSE CNN:   {np.mean(mse_cnn[0]):6.5f} {np.mean(mse_cnn[1]):6.5f} {np.mean(mse_cnn[2]):6.5f} {np.mean(mse_cnn[3]):6.5f}\")\n",
    "print(f\"Mean avgMSE DCNN:  {np.mean(mse_dcnn[0]):6.5f} {np.mean(mse_dcnn[1]):6.5f} {np.mean(mse_dcnn[2]):6.5f} {np.mean(mse_dcnn[3]):6.5f}\")\n",
    "print(f\"Std. avgMSE CNN:   {np.std(mse_cnn[0]):6.5f} {np.std(mse_cnn[1]):6.5f} {np.std(mse_cnn[2]):6.5f} {np.std(mse_cnn[3]):6.5f}\")\n",
    "print(f\"Std. avgMSE DCNN:  {np.std(mse_dcnn[0]):6.5f} {np.std(mse_dcnn[1]):6.5f} {np.std(mse_dcnn[2]):6.5f} {np.std(mse_dcnn[3]):6.5f}\")\n",
    "print(f\"p-value:           {scipy.stats.ttest_rel(mse_cnn[0], mse_dcnn[0])[1]:6.5f} \"+\n",
    "                        f\" {scipy.stats.ttest_rel(mse_cnn[1], mse_dcnn[1])[1]:6.5f} \"+\n",
    "                        f\" {scipy.stats.ttest_rel(mse_cnn[2], mse_dcnn[2])[1]:6.5f} \"+\n",
    "                        f\" {scipy.stats.ttest_rel(mse_cnn[3], mse_dcnn[3])[1]:6.5f}\")\n",
    "\n",
    "print(f\"Mean avgMAE CNN:   {np.mean(mae_cnn[0]):6.5f} {np.mean(mae_cnn[1]):6.5f} {np.mean(mae_cnn[2]):6.5f} {np.mean(mae_cnn[3]):6.5f}\")\n",
    "print(f\"Mean avgMAE DCNN:  {np.mean(mae_dcnn[0]):6.5f} {np.mean(mae_dcnn[1]):6.5f} {np.mean(mae_dcnn[2]):6.5f} {np.mean(mae_dcnn[3]):6.5f}\")\n",
    "print(f\"Std. avgMAE CNN:   {np.std(mae_cnn[0]):6.5f} {np.std(mae_cnn[1]):6.5f} {np.std(mae_cnn[2]):6.5f} {np.std(mae_cnn[3]):6.5f}\")\n",
    "print(f\"Std. avgMAE DCNN:  {np.std(mae_dcnn[0]):6.5f} {np.std(mae_dcnn[1]):6.5f} {np.std(mae_dcnn[2]):6.5f} {np.std(mae_dcnn[3]):6.5f}\")\n",
    "print(f\"p-value:           {scipy.stats.ttest_rel(mae_cnn[0], mae_dcnn[0])[1]:6.5f}\"+\n",
    "                        f\" {scipy.stats.ttest_rel(mae_cnn[1], mae_dcnn[1])[1]:6.5f}\"+\n",
    "                        f\" {scipy.stats.ttest_rel(mae_cnn[2], mae_dcnn[2])[1]:6.5f}\"+\n",
    "                        f\" {scipy.stats.ttest_rel(mae_cnn[3], mae_dcnn[3])[1]:6.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
